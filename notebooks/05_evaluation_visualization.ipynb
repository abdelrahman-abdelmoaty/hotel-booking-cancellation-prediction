{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 5: Model Evaluation and Visualization\n",
        "\n",
        "This notebook provides comprehensive evaluation and visualization of all trained models:\n",
        "1. Detailed performance metrics\n",
        "2. Confusion matrices\n",
        "3. ROC curves comparison\n",
        "4. Feature importance analysis\n",
        "5. Advanced visualizations\n",
        "\n",
        "## Models Evaluated\n",
        "- Naive Bayes\n",
        "- Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install matplotlib seaborn plotly scikit-learn -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"✓ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Model Results\n",
        "\n",
        "**Note**: This notebook assumes you've run the previous ML models notebook. Make sure all models and predictions are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metrics if available\n",
        "try:\n",
        "    metrics_df = pd.read_csv('/content/model_metrics.csv', index_col=0)\n",
        "    print(\"✓ Metrics loaded from CSV\")\n",
        "    display(metrics_df)\n",
        "except:\n",
        "    print(\"⚠ Metrics CSV not found. Make sure to run previous notebook first.\")\n",
        "    print(\"Creating sample metrics structure...\")\n",
        "    metrics_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Performance Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison table\n",
        "if metrics_df is not None:\n",
        "    # Format metrics for display\n",
        "    display_df = metrics_df.copy()\n",
        "    display_df.columns = [col.replace('weighted', '').title() for col in display_df.columns]\n",
        "    \n",
        "    print(\"=== Model Performance Comparison ===\")\n",
        "    display(display_df.round(4))\n",
        "    \n",
        "    # Find best model for each metric\n",
        "    print(\"\\n=== Best Model by Metric ===\")\n",
        "    for metric in metrics_df.columns:\n",
        "        best_model = metrics_df[metric].idxmax()\n",
        "        best_value = metrics_df[metric].max()\n",
        "        print(f\"{metric}: {best_model} ({best_value:.4f})\")\n",
        "else:\n",
        "    print(\"Please run the ML models notebook first to generate metrics.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Visualize Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparing all models\n",
        "if metrics_df is not None:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    metrics_to_plot = ['accuracy', 'auc', 'weightedPrecision', 'weightedRecall', 'f1']\n",
        "    \n",
        "    for i, metric in enumerate(metrics_to_plot[:2]):  # Show first 2 metrics\n",
        "        if metric in metrics_df.columns:\n",
        "            ax = axes[i]\n",
        "            bars = ax.bar(range(len(metrics_df)), metrics_df[metric], \n",
        "                         color=['#e74c3c', '#2ecc71'], alpha=0.7)\n",
        "            ax.set_xticks(range(len(metrics_df)))\n",
        "            ax.set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "            ax.set_title(f'{metric.replace(\"weighted\", \"\").title()} Comparison', fontweight='bold')\n",
        "            ax.set_ylabel('Score')\n",
        "            ax.set_ylim([0, 1])\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # Add value labels on bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional metrics in separate figure\n",
        "    if len(metrics_to_plot) > 2:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        for i, metric in enumerate(metrics_to_plot[2:]):\n",
        "            if metric in metrics_df.columns:\n",
        "                ax = axes[i]\n",
        "                bars = ax.bar(range(len(metrics_df)), metrics_df[metric], \n",
        "                             color=['#e74c3c', '#2ecc71'], alpha=0.7)\n",
        "                ax.set_xticks(range(len(metrics_df)))\n",
        "                ax.set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "                ax.set_title(f'{metric.replace(\"weighted\", \"\").title()} Comparison', fontweight='bold')\n",
        "                ax.set_ylabel('Score')\n",
        "                ax.set_ylim([0, 1])\n",
        "                ax.grid(True, alpha=0.3, axis='y')\n",
        "                \n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Metrics not available for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to extract predictions and labels from Spark DataFrame\n",
        "def get_predictions_labels(spark_predictions):\n",
        "    \"\"\"Extract predictions and labels from Spark DataFrame.\"\"\"\n",
        "    predictions_pd = spark_predictions.select(\"label\", \"prediction\").toPandas()\n",
        "    return predictions_pd['label'].values, predictions_pd['prediction'].values\n",
        "\n",
        "# Create confusion matrices for all models\n",
        "try:\n",
        "    predictions_list = [\n",
        "        (\"Naive Bayes\", nb_predictions),\n",
        "        (\"Decision Tree\", dt_predictions)\n",
        "    ]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (model_name, pred_df) in enumerate(predictions_list):\n",
        "        y_true, y_pred = get_predictions_labels(pred_df)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        \n",
        "        ax = axes[i]\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                   xticklabels=['Not Cancelled', 'Cancelled'],\n",
        "                   yticklabels=['Not Cancelled', 'Cancelled'])\n",
        "        ax.set_title(f'{model_name} - Confusion Matrix', fontweight='bold')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except NameError:\n",
        "    print(\"⚠ Predictions not available. Please run the ML models notebook first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: ROC Curves Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to extract probabilities from Spark predictions\n",
        "def get_probabilities(spark_predictions):\n",
        "    \"\"\"Extract probabilities from Spark DataFrame.\"\"\"\n",
        "    predictions_pd = spark_predictions.select(\"label\", \"probability\").toPandas()\n",
        "    y_true = predictions_pd['label'].values\n",
        "    # Extract probability of positive class (class 1)\n",
        "    y_proba = predictions_pd['probability'].apply(lambda x: x[1]).values\n",
        "    return y_true, y_proba\n",
        "\n",
        "# Plot ROC curves for all models\n",
        "try:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    models_data = [\n",
        "        (\"Naive Bayes\", nb_predictions, '#e74c3c'),\n",
        "        (\"Decision Tree\", dt_predictions, '#2ecc71')\n",
        "    ]\n",
        "    \n",
        "    for model_name, pred_df, color in models_data:\n",
        "        y_true, y_proba = get_probabilities(pred_df)\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        plt.plot(fpr, tpr, color=color, lw=2, \n",
        "                label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except NameError:\n",
        "    print(\"⚠ Predictions not available. Please run the ML models notebook first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Interactive Visualization with Plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive metrics comparison\n",
        "if metrics_df is not None:\n",
        "    # Prepare data for Plotly\n",
        "    metrics_long = metrics_df.reset_index().melt(\n",
        "        id_vars='model',\n",
        "        var_name='metric',\n",
        "        value_name='score'\n",
        "    )\n",
        "    \n",
        "    # Create interactive bar chart\n",
        "    fig = px.bar(metrics_long, x='model', y='score', color='metric',\n",
        "                 barmode='group', title='Model Performance Metrics Comparison',\n",
        "                 labels={'score': 'Score', 'model': 'Model', 'metric': 'Metric'})\n",
        "    fig.update_layout(height=600, showlegend=True)\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"Metrics not available for interactive visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and visualize feature importance from tree-based models\n",
        "try:\n",
        "    # Decision Tree feature importance\n",
        "    dt_importance = dt_model.featureImportances.toArray()\n",
        "    \n",
        "    # Create feature importance visualization\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "    \n",
        "    # Decision Tree\n",
        "    top_dt_indices = np.argsort(dt_importance)[-15:][::-1]\n",
        "    ax.barh(range(len(top_dt_indices)), dt_importance[top_dt_indices], color='coral', alpha=0.7)\n",
        "    ax.set_yticks(range(len(top_dt_indices)))\n",
        "    ax.set_yticklabels([f'Feature {idx}' for idx in top_dt_indices])\n",
        "    ax.set_title('Decision Tree - Top 15 Feature Importance', fontweight='bold', fontsize=14)\n",
        "    ax.set_xlabel('Importance', fontsize=12)\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except NameError:\n",
        "    print(\"⚠ Models not available. Please run the ML models notebook first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Performance Summary Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive summary\n",
        "if metrics_df is not None:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(\"\\n1. ACCURACY RANKING:\")\n",
        "    accuracy_rank = metrics_df['accuracy'].sort_values(ascending=False)\n",
        "    for i, (model, score) in enumerate(accuracy_rank.items(), 1):\n",
        "        print(f\"   {i}. {model}: {score:.4f}\")\n",
        "    \n",
        "    print(\"\\n2. AUC-ROC RANKING:\")\n",
        "    auc_rank = metrics_df['auc'].sort_values(ascending=False)\n",
        "    for i, (model, score) in enumerate(auc_rank.items(), 1):\n",
        "        print(f\"   {i}. {model}: {score:.4f}\")\n",
        "    \n",
        "    print(\"\\n3. F1-SCORE RANKING:\")\n",
        "    f1_rank = metrics_df['f1'].sort_values(ascending=False)\n",
        "    for i, (model, score) in enumerate(f1_rank.items(), 1):\n",
        "        print(f\"   {i}. {model}: {score:.4f}\")\n",
        "    \n",
        "    print(\"\\n4. BEST OVERALL MODEL:\")\n",
        "    # Calculate average rank across all metrics\n",
        "    ranks = pd.DataFrame({\n",
        "        'accuracy': metrics_df['accuracy'].rank(ascending=False),\n",
        "        'auc': metrics_df['auc'].rank(ascending=False),\n",
        "        'f1': metrics_df['f1'].rank(ascending=False),\n",
        "        'weightedPrecision': metrics_df['weightedPrecision'].rank(ascending=False),\n",
        "        'weightedRecall': metrics_df['weightedRecall'].rank(ascending=False)\n",
        "    })\n",
        "    avg_rank = ranks.mean(axis=1)\n",
        "    best_model = avg_rank.idxmin()\n",
        "    print(f\"   {best_model} (Average Rank: {avg_rank[best_model]:.2f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "else:\n",
        "    print(\"Metrics not available for summary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Visualizations\n",
        "\n",
        "Save all visualizations for the final report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create reports directory\n",
        "import os\n",
        "os.makedirs('/content/reports/figures', exist_ok=True)\n",
        "\n",
        "# Save metrics table\n",
        "if metrics_df is not None:\n",
        "    metrics_df.to_csv('/content/reports/model_metrics_final.csv')\n",
        "    print(\"✓ Metrics saved to reports/model_metrics_final.csv\")\n",
        "\n",
        "print(\"\\n✓ All visualizations completed\")\n",
        "print(\"\\nVisualizations can be saved by:\")\n",
        "print(\"  1. Right-clicking on plots and selecting 'Save image'\")\n",
        "print(\"  2. Using plt.savefig() in code cells\")\n",
        "print(\"  3. Downloading from Colab file browser\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✓ Model performance comparison completed\n",
        "✓ Confusion matrices generated\n",
        "✓ ROC curves compared\n",
        "✓ Feature importance analyzed\n",
        "✓ Interactive visualizations created\n",
        "✓ Performance summary dashboard generated\n",
        "\n",
        "**Project Complete!** All components have been implemented:\n",
        "- ✓ Data ingestion into MongoDB\n",
        "- ✓ Exploratory Data Analysis\n",
        "- ✓ Spark data processing\n",
        "- ✓ ML models trained (Naive Bayes, Decision Tree)\n",
        "- ✓ Comprehensive evaluation and visualization\n",
        "\n",
        "**Next Steps**: \n",
        "- Review all results\n",
        "- Prepare final report with screenshots\n",
        "- Document team contributions\n",
        "- Upload to GitHub\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
