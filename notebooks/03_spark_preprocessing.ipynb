{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Spark Data Processing and Feature Engineering\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Setting up PySpark in Colab\n",
        "2. Loading data into Spark DataFrame\n",
        "3. Feature engineering using Spark transformations\n",
        "4. Data preprocessing pipeline\n",
        "5. Preparing data for machine learning\n",
        "\n",
        "## Why Spark?\n",
        "- Distributed processing for large datasets\n",
        "- Scalable feature engineering\n",
        "- MLlib for machine learning\n",
        "- Handles data beyond memory limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PySpark\n",
        "%pip install pyspark findspark -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, isnan, isnull, count, avg, sum as spark_sum\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, Imputer, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HotelBookingPreprocessing\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"✓ Spark session created\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl if hasattr(spark.sparkContext, 'uiWebUrl') else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Data into Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Load from CSV\n",
        "csv_path = \"/content/hotel_bookings.csv\"  # Colab path\n",
        "# csv_path = \"../data/hotel_bookings.csv\"  # Local path\n",
        "\n",
        "# Option 2: Load from MongoDB (requires MongoDB Spark Connector)\n",
        "# For simplicity, we'll load from CSV and demonstrate MongoDB integration conceptually\n",
        "\n",
        "# Load CSV into Spark DataFrame\n",
        "df_spark = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "\n",
        "print(f\"✓ Data loaded into Spark\")\n",
        "print(f\"Number of partitions: {df_spark.rdd.getNumPartitions()}\")\n",
        "print(f\"Total records: {df_spark.count():,}\")\n",
        "print(f\"\\nSchema:\")\n",
        "df_spark.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first few rows\n",
        "print(\"=== Sample Data ===\")\n",
        "df_spark.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Quality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values Analysis ===\")\n",
        "\n",
        "# Use isnull() for all columns (works for all data types: numeric, string, date, etc.)\n",
        "# For numeric columns, NaN values are also considered null in Spark\n",
        "missing_counts = df_spark.select([count(when(isnull(col(c)), c)).alias(c) \n",
        "                                   for c in df_spark.columns])\n",
        "\n",
        "# Convert to pandas for better display\n",
        "missing_pd = missing_counts.toPandas().T\n",
        "missing_pd.columns = ['Missing Count']\n",
        "missing_pd = missing_pd[missing_pd['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_pd) > 0:\n",
        "    print(\"Columns with missing values:\")\n",
        "    display(missing_pd)\n",
        "else:\n",
        "    print(\"✓ No missing values found\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"=== Basic Statistics ===\")\n",
        "df_spark.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Engineering\n",
        "\n",
        "Create new features using Spark transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 1: Total nights\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_nights\",\n",
        "    col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")\n",
        ")\n",
        "\n",
        "# Feature 2: Total guests\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_guests\",\n",
        "    col(\"adults\") + \n",
        "    when(col(\"children\").isNull(), 0).otherwise(col(\"children\")) +\n",
        "    when(col(\"babies\").isNull(), 0).otherwise(col(\"babies\"))\n",
        ")\n",
        "\n",
        "# Feature 3: Total booking value (ADR × total nights)\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_booking_value\",\n",
        "    col(\"adr\") * col(\"total_nights\")\n",
        ")\n",
        "\n",
        "# Feature 4: Lead time categories\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"lead_time_category\",\n",
        "    when(col(\"lead_time\") <= 30, \"Very Short\")\n",
        "    .when(col(\"lead_time\") <= 90, \"Short\")\n",
        "    .when(col(\"lead_time\") <= 180, \"Medium\")\n",
        "    .when(col(\"lead_time\") <= 365, \"Long\")\n",
        "    .otherwise(\"Very Long\")\n",
        ")\n",
        "\n",
        "# Feature 5: Previous cancellation ratio\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"previous_cancellation_ratio\",\n",
        "    col(\"previous_cancellations\") / \n",
        "    (col(\"previous_cancellations\") + col(\"previous_bookings_not_canceled\") + 1)\n",
        ")\n",
        "\n",
        "print(\"✓ Feature engineering completed\")\n",
        "print(\"\\nNew features created:\")\n",
        "print(\"  - total_nights\")\n",
        "print(\"  - total_guests\")\n",
        "print(\"  - total_booking_value\")\n",
        "print(\"  - lead_time_category\")\n",
        "print(\"  - previous_cancellation_ratio\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify new features\n",
        "print(\"=== Sample of New Features ===\")\n",
        "df_spark.select(\"total_nights\", \"total_guests\", \"total_booking_value\", \n",
        "                \"lead_time_category\", \"previous_cancellation_ratio\").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "# Handle both null values and 'NA' strings\n",
        "# Children\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"children\",\n",
        "    when(col(\"children\").isNull(), 0)\n",
        "    .when(col(\"children\").cast(\"string\") == \"NA\", 0)\n",
        "    .otherwise(col(\"children\").cast(\"double\"))\n",
        ")\n",
        "\n",
        "# Agent\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"agent\",\n",
        "    when(col(\"agent\").isNull(), 0)\n",
        "    .when(col(\"agent\").cast(\"string\") == \"NA\", 0)\n",
        "    .otherwise(col(\"agent\").cast(\"double\"))\n",
        ")\n",
        "\n",
        "# Company\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"company\",\n",
        "    when(col(\"company\").isNull(), 0)\n",
        "    .when(col(\"company\").cast(\"string\") == \"NA\", 0)\n",
        "    .otherwise(col(\"company\").cast(\"double\"))\n",
        ")\n",
        "\n",
        "# Country (string column - handle differently)\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"country\",\n",
        "    when(col(\"country\").isNull(), \"Unknown\")\n",
        "    .when(col(\"country\") == \"NA\", \"Unknown\")\n",
        "    .otherwise(col(\"country\"))\n",
        ")\n",
        "\n",
        "# CRITICAL: Clean is_canceled column early to avoid issues later\n",
        "# This is the target variable and must be integer type with no 'NA' strings\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"is_canceled\",\n",
        "    when(col(\"is_canceled\").cast(\"string\").isin([\"NA\", \"na\", \"Na\", \"nA\", \"null\", \"NULL\", \"\"]), 0)\n",
        "    .when(col(\"is_canceled\").isNull(), 0)\n",
        "    .otherwise(col(\"is_canceled\").cast(\"integer\"))\n",
        ")\n",
        "\n",
        "print(\"✓ Missing values handled (including 'NA' strings)\")\n",
        "print(\"✓ Target variable (is_canceled) cleaned and cast to integer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare Features for ML\n",
        "\n",
        "Select and prepare features for machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for ML\n",
        "# Numerical features\n",
        "numerical_features = [\n",
        "    \"lead_time\", \"arrival_date_year\", \"arrival_date_week_number\",\n",
        "    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\", \n",
        "    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n",
        "    \"previous_cancellations\", \"previous_bookings_not_canceled\",\n",
        "    \"adr\", \"required_car_parking_spaces\", \"total_of_special_requests\",\n",
        "    \"total_nights\", \"total_guests\", \"total_booking_value\",\n",
        "    \"previous_cancellation_ratio\"\n",
        "]\n",
        "\n",
        "# Categorical features to encode\n",
        "categorical_features = [\n",
        "    \"hotel\", \"meal\", \"country\", \"market_segment\",\n",
        "    \"distribution_channel\", \"reserved_room_type\",\n",
        "    \"assigned_room_type\", \"deposit_type\", \"customer_type\",\n",
        "    \"lead_time_category\"\n",
        "]\n",
        "\n",
        "# Filter to existing columns\n",
        "numerical_features = [f for f in numerical_features if f in df_spark.columns]\n",
        "categorical_features = [f for f in categorical_features if f in df_spark.columns]\n",
        "\n",
        "print(f\"Numerical features: {len(numerical_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create StringIndexers for categorical features\n",
        "indexers = []\n",
        "for col_name in categorical_features:\n",
        "    indexer = StringIndexer(\n",
        "        inputCol=col_name,\n",
        "        outputCol=f\"{col_name}_indexed\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    indexers.append(indexer)\n",
        "\n",
        "print(f\"✓ Created {len(indexers)} StringIndexers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply StringIndexers\n",
        "df_indexed = df_spark\n",
        "for indexer in indexers:\n",
        "    df_indexed = indexer.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "print(\"✓ Categorical features indexed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare feature columns for VectorAssembler\n",
        "feature_columns = numerical_features + [f\"{col}_indexed\" for col in categorical_features]\n",
        "\n",
        "# Final cleaning: Aggressively clean 'NA' strings and ensure proper types\n",
        "df_cleaned = df_indexed\n",
        "\n",
        "# Clean each numerical column: replace 'NA' strings and ensure double type\n",
        "for col_name in numerical_features:\n",
        "    if col_name in df_cleaned.columns:\n",
        "        # Step 1: Convert to string, replace 'NA' variants with null, then cast to double\n",
        "        df_cleaned = df_cleaned.withColumn(\n",
        "            col_name + \"_temp\",\n",
        "            when(col(col_name).cast(\"string\").isin([\"NA\", \"na\", \"Na\", \"nA\", \"null\", \"NULL\", \"Null\", \"\", \"None\"]), None)\n",
        "            .otherwise(col(col_name))\n",
        "        ).withColumn(\n",
        "            col_name,\n",
        "            col(col_name + \"_temp\").cast(\"double\")\n",
        "        ).drop(col_name + \"_temp\")\n",
        "\n",
        "print(\"✓ Cleaned 'NA' strings and ensured numerical columns are double type\")\n",
        "\n",
        "# Handle missing values manually - calculate mean on clean data\n",
        "df_imputed = df_cleaned\n",
        "\n",
        "print(\"Calculating means and filling missing values...\")\n",
        "for i, col_name in enumerate(numerical_features):\n",
        "    if col_name in df_imputed.columns:\n",
        "        try:\n",
        "            # Calculate mean (this will work now that 'NA' strings are null)\n",
        "            mean_result = df_imputed.select(avg(col(col_name)).alias(\"mean\")).collect()[0]\n",
        "            mean_value = mean_result[\"mean\"] if mean_result[\"mean\"] is not None else 0.0\n",
        "            \n",
        "            # Fill null values with mean\n",
        "            df_imputed = df_imputed.withColumn(\n",
        "                f\"{col_name}_imputed\",\n",
        "                when(col(col_name).isNull(), mean_value)\n",
        "                .otherwise(col(col_name))\n",
        "            )\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(numerical_features)} columns...\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not process {col_name}: {e}\")\n",
        "            # If mean calculation fails, just use 0.0\n",
        "            df_imputed = df_imputed.withColumn(\n",
        "                f\"{col_name}_imputed\",\n",
        "                when(col(col_name).isNull(), 0.0)\n",
        "                .otherwise(col(col_name))\n",
        "            )\n",
        "\n",
        "print(\"✓ Missing values filled with column means\")\n",
        "\n",
        "# Update feature columns to use imputed versions\n",
        "feature_columns_imputed = [f\"{col}_imputed\" for col in numerical_features] + \\\n",
        "                         [f\"{col}_indexed\" for col in categorical_features]\n",
        "\n",
        "print(f\"✓ Features prepared: {len(feature_columns_imputed)} total features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create Feature Vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_columns_imputed,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "df_features = assembler.transform(df_imputed)\n",
        "\n",
        "print(\"✓ Features assembled into vector\")\n",
        "print(f\"Feature vector size: {len(feature_columns_imputed)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select final columns for ML\n",
        "# Use try_cast to safely handle 'NA' strings without errors\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Step 1: Use try_cast to safely convert is_canceled to integer\n",
        "# try_cast returns NULL if casting fails, which we can then handle\n",
        "df_ml = df_features.withColumn(\n",
        "    \"label_temp\",\n",
        "    expr(\"try_cast(is_canceled as int)\")\n",
        ").withColumn(\n",
        "    \"label\",\n",
        "    when(col(\"label_temp\").isNull(), 0)\n",
        "    .otherwise(col(\"label_temp\"))\n",
        ").select(\"features\", \"label\")\n",
        "\n",
        "# Step 2: Filter out any rows where features might be problematic\n",
        "# (This shouldn't be needed if previous steps worked, but just in case)\n",
        "df_ml = df_ml.filter(col(\"label\").isNotNull())\n",
        "\n",
        "# Cache for faster access (only after data is clean)\n",
        "df_ml.cache()\n",
        "\n",
        "print(\"✓ Data prepared for ML\")\n",
        "print(f\"Total records: {df_ml.count():,}\")\n",
        "df_ml.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets (80/20)\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"=== Train/Test Split ===\")\n",
        "print(f\"Training set: {train_df.count():,} records ({train_df.count()/df_ml.count()*100:.1f}%)\")\n",
        "print(f\"Test set: {test_df.count():,} records ({test_df.count()/df_ml.count()*100:.1f}%)\")\n",
        "\n",
        "# Check label distribution in both sets\n",
        "print(\"\\n=== Label Distribution ===\")\n",
        "print(\"Training set:\")\n",
        "train_df.groupBy(\"label\").count().show()\n",
        "print(\"\\nTest set:\")\n",
        "test_df.groupBy(\"label\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Processed Data (Optional)\n",
        "\n",
        "Save processed data for use in next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: In production, you might save to Parquet for efficient storage\n",
        "# For this project, we'll keep data in memory and use in next notebook\n",
        "\n",
        "print(\"✓ Data preprocessing completed\")\n",
        "print(\"\\nReady for machine learning models!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  - Use train_df and test_df for model training\")\n",
        "print(\"  - Features are in 'features' column\")\n",
        "print(\"  - Labels are in 'label' column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✓ Spark session initialized\n",
        "✓ Data loaded and cached\n",
        "✓ Feature engineering completed\n",
        "✓ Missing values handled\n",
        "✓ Categorical features encoded\n",
        "✓ Features assembled into vectors\n",
        "✓ Train/test split performed\n",
        "\n",
        "**Next Steps**: Proceed to `04_ml_models.ipynb` to train machine learning models.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
