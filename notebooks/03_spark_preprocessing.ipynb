{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Spark Data Processing and Feature Engineering\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Setting up PySpark\n",
        "2. Loading data into Spark DataFrame\n",
        "3. Feature engineering using Spark transformations\n",
        "4. Data preprocessing pipeline\n",
        "5. Preparing data for machine learning\n",
        "\n",
        "## Why Spark?\n",
        "- Distributed processing for large datasets\n",
        "- Scalable feature engineering\n",
        "- MLlib for machine learning\n",
        "- Handles data beyond memory limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "%pip install pyspark findspark -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, isnan, isnull, count, avg, sum as spark_sum\n",
        "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, Imputer, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Spark session created\n",
            "Spark version: 4.1.0\n",
            "Spark UI: http://192.168.1.18:4040\n"
          ]
        }
      ],
      "source": [
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HotelBookingPreprocessing\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"✓ Spark session created\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl if hasattr(spark.sparkContext, 'uiWebUrl') else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Data into Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data loaded into Spark\n",
            "Number of partitions: 5\n",
            "Total records: 119,390\n",
            "\n",
            "Schema:\n",
            "root\n",
            " |-- hotel: string (nullable = true)\n",
            " |-- is_canceled: integer (nullable = true)\n",
            " |-- lead_time: integer (nullable = true)\n",
            " |-- arrival_date_year: integer (nullable = true)\n",
            " |-- arrival_date_month: string (nullable = true)\n",
            " |-- arrival_date_week_number: integer (nullable = true)\n",
            " |-- arrival_date_day_of_month: integer (nullable = true)\n",
            " |-- stays_in_weekend_nights: integer (nullable = true)\n",
            " |-- stays_in_week_nights: integer (nullable = true)\n",
            " |-- adults: integer (nullable = true)\n",
            " |-- children: integer (nullable = true)\n",
            " |-- babies: integer (nullable = true)\n",
            " |-- meal: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- market_segment: string (nullable = true)\n",
            " |-- distribution_channel: string (nullable = true)\n",
            " |-- is_repeated_guest: integer (nullable = true)\n",
            " |-- previous_cancellations: integer (nullable = true)\n",
            " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
            " |-- reserved_room_type: string (nullable = true)\n",
            " |-- assigned_room_type: string (nullable = true)\n",
            " |-- booking_changes: integer (nullable = true)\n",
            " |-- deposit_type: string (nullable = true)\n",
            " |-- agent: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- days_in_waiting_list: integer (nullable = true)\n",
            " |-- customer_type: string (nullable = true)\n",
            " |-- adr: double (nullable = true)\n",
            " |-- required_car_parking_spaces: integer (nullable = true)\n",
            " |-- total_of_special_requests: integer (nullable = true)\n",
            " |-- reservation_status: string (nullable = true)\n",
            " |-- reservation_status_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load from CSV\n",
        "csv_path = \"../data/hotel_bookings.csv\"\n",
        "\n",
        "# Option: Load from MongoDB (requires MongoDB Spark Connector)\n",
        "# For simplicity, we'll load from CSV and demonstrate MongoDB integration conceptually\n",
        "\n",
        "# Load CSV into Spark DataFrame\n",
        "# ⚠️ CRITICAL: Use nullValue=\"NA\" to treat 'NA' strings as null values during load\n",
        "# This prevents casting errors later in the pipeline\n",
        "df_spark = spark.read.csv(\n",
        "    csv_path, \n",
        "    header=True, \n",
        "    inferSchema=True,\n",
        "    nullValue=\"NA\"  # This converts 'NA' strings to null at source\n",
        ")\n",
        "\n",
        "print(f\"✓ Data loaded into Spark\")\n",
        "print(f\"Number of partitions: {df_spark.rdd.getNumPartitions()}\")\n",
        "print(f\"Total records: {df_spark.count():,}\")\n",
        "print(f\"\\nSchema:\")\n",
        "df_spark.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sample Data ===\n",
            "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+\n",
            "|hotel       |is_canceled|lead_time|arrival_date_year|arrival_date_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|adults|children|babies|meal|country|market_segment|distribution_channel|is_repeated_guest|previous_cancellations|previous_bookings_not_canceled|reserved_room_type|assigned_room_type|booking_changes|deposit_type|agent|company|days_in_waiting_list|customer_type|adr |required_car_parking_spaces|total_of_special_requests|reservation_status|reservation_status_date|\n",
            "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+\n",
            "|Resort Hotel|0          |342      |2015             |July              |27                      |1                        |0                      |0                   |2     |0       |0     |BB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |3              |No Deposit  |NULL |NULL   |0                   |Transient    |0.0 |0                          |0                        |Check-Out         |2015-07-01             |\n",
            "|Resort Hotel|0          |737      |2015             |July              |27                      |1                        |0                      |0                   |2     |0       |0     |BB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |4              |No Deposit  |NULL |NULL   |0                   |Transient    |0.0 |0                          |0                        |Check-Out         |2015-07-01             |\n",
            "|Resort Hotel|0          |7        |2015             |July              |27                      |1                        |0                      |1                   |1     |0       |0     |BB  |GBR    |Direct        |Direct              |0                |0                     |0                             |A                 |C                 |0              |No Deposit  |NULL |NULL   |0                   |Transient    |75.0|0                          |0                        |Check-Out         |2015-07-02             |\n",
            "|Resort Hotel|0          |13       |2015             |July              |27                      |1                        |0                      |1                   |1     |0       |0     |BB  |GBR    |Corporate     |Corporate           |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |304  |NULL   |0                   |Transient    |75.0|0                          |0                        |Check-Out         |2015-07-02             |\n",
            "|Resort Hotel|0          |14       |2015             |July              |27                      |1                        |0                      |2                   |2     |0       |0     |BB  |GBR    |Online TA     |TA/TO               |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |240  |NULL   |0                   |Transient    |98.0|0                          |1                        |Check-Out         |2015-07-03             |\n",
            "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+\n",
            "only showing top 5 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/30 16:48:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
          ]
        }
      ],
      "source": [
        "# Show first few rows\n",
        "print(\"=== Sample Data ===\")\n",
        "df_spark.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Quality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Missing Values Analysis ===\n",
            "Columns with missing values:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>children</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Missing Count\n",
              "children              4"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values Analysis ===\")\n",
        "\n",
        "# Use isnull() for all columns (works for all data types: numeric, string, date, etc.)\n",
        "# For numeric columns, NaN values are also considered null in Spark\n",
        "missing_counts = df_spark.select([count(when(isnull(col(c)), c)).alias(c) \n",
        "                                   for c in df_spark.columns])\n",
        "\n",
        "# Convert to pandas for better display\n",
        "missing_pd = missing_counts.toPandas().T\n",
        "missing_pd.columns = ['Missing Count']\n",
        "missing_pd = missing_pd[missing_pd['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_pd) > 0:\n",
        "    print(\"Columns with missing values:\")\n",
        "    display(missing_pd)\n",
        "else:\n",
        "    print(\"✓ No missing values found\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Basic Statistics ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:===========>                                               (1 + 4) / 5]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------+-------------------+------------------+------------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------------------+-------------------+--------------------+---------+-------+--------------+--------------------+-------------------+----------------------+------------------------------+------------------+------------------+-------------------+------------+-----------------+------------------+--------------------+---------------+------------------+---------------------------+-------------------------+------------------+\n",
            "|summary|       hotel|        is_canceled|         lead_time| arrival_date_year|arrival_date_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|            adults|           children|              babies|     meal|country|market_segment|distribution_channel|  is_repeated_guest|previous_cancellations|previous_bookings_not_canceled|reserved_room_type|assigned_room_type|    booking_changes|deposit_type|            agent|           company|days_in_waiting_list|  customer_type|               adr|required_car_parking_spaces|total_of_special_requests|reservation_status|\n",
            "+-------+------------+-------------------+------------------+------------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------------------+-------------------+--------------------+---------+-------+--------------+--------------------+-------------------+----------------------+------------------------------+------------------+------------------+-------------------+------------+-----------------+------------------+--------------------+---------------+------------------+---------------------------+-------------------------+------------------+\n",
            "|  count|      119390|             119390|            119390|            119390|            119390|                  119390|                   119390|                 119390|              119390|            119390|             119386|              119390|   119390| 119390|        119390|              119390|             119390|                119390|                        119390|            119390|            119390|             119390|      119390|           119390|            119390|              119390|         119390|            119390|                     119390|                   119390|            119390|\n",
            "|   mean|        NULL|0.37041628277075134|104.01141636652986| 2016.156554150264|              NULL|       27.16517296255968|       15.798241058715135|     0.9275986263506156|   2.500301532791691|1.8564033838679956|0.10388990333874994|0.007948739425412514|     NULL|   NULL|          NULL|                NULL|0.03191222045397437|   0.08711784906608594|           0.13709690928888515|              NULL|              NULL|0.22112404724013737|        NULL|86.69338185346919|189.26673532440782|   2.321149174972778|           NULL|101.83112153446673|        0.06251779881062065|       0.5713627607002262|              NULL|\n",
            "| stddev|        NULL|0.48291822659259825|106.86309704798795|0.7074759445193511|              NULL|        13.6051383554977|        8.780829470578352|     0.9986134945978756|  1.9082856150479124|0.5792609988327547|0.39856144478644184| 0.09743619130126424|     NULL|   NULL|          NULL|                NULL| 0.1757671454106565|    0.8443363841545128|            1.4974368477076807|              NULL|              NULL| 0.6523055726747704|        NULL|110.7745476429513| 131.6550146385122|   17.59472087877625|           NULL| 50.53579028554864|         0.2452911474674931|       0.7927984228094103|              NULL|\n",
            "|    min|  City Hotel|                  0|                 0|              2015|             April|                       1|                        1|                      0|                   0|                 0|                  0|                   0|       BB|    ABW|      Aviation|           Corporate|                  0|                     0|                             0|                 A|                 A|                  0|  No Deposit|                1|                10|                   0|       Contract|             -6.38|                          0|                        0|          Canceled|\n",
            "|    max|Resort Hotel|                  1|               737|              2017|         September|                      53|                       31|                     19|                  50|                55|                 10|                  10|Undefined|    ZWE|     Undefined|           Undefined|                  1|                    26|                            72|                 P|                 P|                 21|  Refundable|             NULL|              NULL|                 391|Transient-Party|            5400.0|                          8|                        5|           No-Show|\n",
            "+-------+------------+-------------------+------------------+------------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------------------+-------------------+--------------------+---------+-------+--------------+--------------------+-------------------+----------------------+------------------------------+------------------+------------------+-------------------+------------+-----------------+------------------+--------------------+---------------+------------------+---------------------------+-------------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Basic statistics\n",
        "print(\"=== Basic Statistics ===\")\n",
        "df_spark.describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Engineering\n",
        "\n",
        "Create new features using Spark transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Feature engineering completed\n",
            "\n",
            "New features created:\n",
            "  - total_nights\n",
            "  - total_guests\n",
            "  - total_booking_value\n",
            "  - lead_time_category\n",
            "  - previous_cancellation_ratio\n"
          ]
        }
      ],
      "source": [
        "# Feature 1: Total nights\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_nights\",\n",
        "    col(\"stays_in_weekend_nights\") + col(\"stays_in_week_nights\")\n",
        ")\n",
        "\n",
        "# Feature 2: Total guests\n",
        "# Use try_cast to safely handle 'NA' strings, then coalesce for nulls\n",
        "from pyspark.sql.functions import coalesce, lit, expr\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"children_clean\",\n",
        "    expr(\"try_cast(children as double)\")  # Safely cast, returns null if 'NA'\n",
        ").withColumn(\n",
        "    \"babies_clean\",\n",
        "    expr(\"try_cast(babies as double)\")  # Safely cast, returns null if 'NA'\n",
        ").withColumn(\n",
        "    \"total_guests\",\n",
        "    coalesce(col(\"adults\"), lit(0)) + \n",
        "    coalesce(col(\"children_clean\"), lit(0.0)) +\n",
        "    coalesce(col(\"babies_clean\"), lit(0.0))\n",
        ").drop(\"children_clean\", \"babies_clean\")  # Drop temporary columns\n",
        "\n",
        "# Feature 3: Total booking value (ADR × total nights)\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"total_booking_value\",\n",
        "    col(\"adr\") * col(\"total_nights\")\n",
        ")\n",
        "\n",
        "# Feature 4: Lead time categories\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"lead_time_category\",\n",
        "    when(col(\"lead_time\") <= 30, \"Very Short\")\n",
        "    .when(col(\"lead_time\") <= 90, \"Short\")\n",
        "    .when(col(\"lead_time\") <= 180, \"Medium\")\n",
        "    .when(col(\"lead_time\") <= 365, \"Long\")\n",
        "    .otherwise(\"Very Long\")\n",
        ")\n",
        "\n",
        "# Feature 5: Previous cancellation ratio\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"previous_cancellation_ratio\",\n",
        "    col(\"previous_cancellations\") / \n",
        "    (col(\"previous_cancellations\") + col(\"previous_bookings_not_canceled\") + 1)\n",
        ")\n",
        "\n",
        "print(\"✓ Feature engineering completed\")\n",
        "print(\"\\nNew features created:\")\n",
        "print(\"  - total_nights\")\n",
        "print(\"  - total_guests\")\n",
        "print(\"  - total_booking_value\")\n",
        "print(\"  - lead_time_category\")\n",
        "print(\"  - previous_cancellation_ratio\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sample of New Features ===\n",
            "+------------+------------+-------------------+------------------+---------------------------+\n",
            "|total_nights|total_guests|total_booking_value|lead_time_category|previous_cancellation_ratio|\n",
            "+------------+------------+-------------------+------------------+---------------------------+\n",
            "|           0|         2.0|                0.0|              Long|                        0.0|\n",
            "|           0|         2.0|                0.0|         Very Long|                        0.0|\n",
            "|           1|         1.0|               75.0|        Very Short|                        0.0|\n",
            "|           1|         1.0|               75.0|        Very Short|                        0.0|\n",
            "|           2|         2.0|              196.0|        Very Short|                        0.0|\n",
            "|           2|         2.0|              196.0|        Very Short|                        0.0|\n",
            "|           2|         2.0|              214.0|        Very Short|                        0.0|\n",
            "|           2|         2.0|              206.0|        Very Short|                        0.0|\n",
            "|           3|         2.0|              246.0|             Short|                        0.0|\n",
            "|           3|         2.0|              316.5|             Short|                        0.0|\n",
            "+------------+------------+-------------------+------------------+---------------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Verify new features\n",
        "print(\"=== Sample of New Features ===\")\n",
        "df_spark.select(\"total_nights\", \"total_guests\", \"total_booking_value\", \n",
        "                \"lead_time_category\", \"previous_cancellation_ratio\").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Missing values handled (including 'NA' strings)\n",
            "✓ Target variable (is_canceled) cleaned and cast to integer\n"
          ]
        }
      ],
      "source": [
        "# Fill missing values\n",
        "# Handle both null values and 'NA' strings using try_cast for safety\n",
        "from pyspark.sql.functions import expr, coalesce, lit\n",
        "\n",
        "# Children\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"children\",\n",
        "    coalesce(expr(\"try_cast(children as double)\"), lit(0.0))  # try_cast returns null if 'NA', then coalesce to 0\n",
        ")\n",
        "\n",
        "# Agent\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"agent\",\n",
        "    coalesce(expr(\"try_cast(agent as double)\"), lit(0.0))  # try_cast returns null if 'NA', then coalesce to 0\n",
        ")\n",
        "\n",
        "# Company\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"company\",\n",
        "    coalesce(expr(\"try_cast(company as double)\"), lit(0.0))  # try_cast returns null if 'NA', then coalesce to 0\n",
        ")\n",
        "\n",
        "# Country (string column - handle differently)\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"country\",\n",
        "    when(col(\"country\").isNull(), \"Unknown\")\n",
        "    .when(col(\"country\") == \"NA\", \"Unknown\")\n",
        "    .otherwise(col(\"country\"))\n",
        ")\n",
        "\n",
        "# CRITICAL: Clean is_canceled column early to avoid issues later\n",
        "# This is the target variable and must be integer type with no 'NA' strings\n",
        "df_spark = df_spark.withColumn(\n",
        "    \"is_canceled\",\n",
        "    when(col(\"is_canceled\").cast(\"string\").isin([\"NA\", \"na\", \"Na\", \"nA\", \"null\", \"NULL\", \"\"]), 0)\n",
        "    .when(col(\"is_canceled\").isNull(), 0)\n",
        "    .otherwise(col(\"is_canceled\").cast(\"integer\"))\n",
        ")\n",
        "\n",
        "print(\"✓ Missing values handled (including 'NA' strings)\")\n",
        "print(\"✓ Target variable (is_canceled) cleaned and cast to integer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare Features for ML\n",
        "\n",
        "Select and prepare features for machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical features: 18\n",
            "Categorical features: 10\n"
          ]
        }
      ],
      "source": [
        "# Select features for ML\n",
        "# Numerical features\n",
        "numerical_features = [\n",
        "    \"lead_time\", \"arrival_date_year\", \"arrival_date_week_number\",\n",
        "    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\", \n",
        "    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n",
        "    \"previous_cancellations\", \"previous_bookings_not_canceled\",\n",
        "    \"adr\", \"required_car_parking_spaces\", \"total_of_special_requests\",\n",
        "    \"total_nights\", \"total_guests\", \"total_booking_value\",\n",
        "    \"previous_cancellation_ratio\"\n",
        "]\n",
        "\n",
        "# Categorical features to encode\n",
        "categorical_features = [\n",
        "    \"hotel\", \"meal\", \"country\", \"market_segment\",\n",
        "    \"distribution_channel\", \"reserved_room_type\",\n",
        "    \"assigned_room_type\", \"deposit_type\", \"customer_type\",\n",
        "    \"lead_time_category\"\n",
        "]\n",
        "\n",
        "# Filter to existing columns\n",
        "numerical_features = [f for f in numerical_features if f in df_spark.columns]\n",
        "categorical_features = [f for f in categorical_features if f in df_spark.columns]\n",
        "\n",
        "print(f\"Numerical features: {len(numerical_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Created 10 StringIndexers\n"
          ]
        }
      ],
      "source": [
        "# Create StringIndexers for categorical features\n",
        "indexers = []\n",
        "for col_name in categorical_features:\n",
        "    indexer = StringIndexer(\n",
        "        inputCol=col_name,\n",
        "        outputCol=f\"{col_name}_indexed\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    indexers.append(indexer)\n",
        "\n",
        "print(f\"✓ Created {len(indexers)} StringIndexers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Categorical features indexed\n"
          ]
        }
      ],
      "source": [
        "# Apply StringIndexers\n",
        "df_indexed = df_spark\n",
        "for indexer in indexers:\n",
        "    df_indexed = indexer.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "print(\"✓ Categorical features indexed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Cleaned 'NA' strings and ensured numerical columns are double type\n",
            "Calculating means and filling missing values...\n",
            "  Processed 5/18 columns...\n",
            "  Processed 10/18 columns...\n",
            "  Processed 15/18 columns...\n",
            "✓ Missing values filled with column means\n",
            "✓ Features prepared: 28 total features\n"
          ]
        }
      ],
      "source": [
        "# Prepare feature columns for VectorAssembler\n",
        "feature_columns = numerical_features + [f\"{col}_indexed\" for col in categorical_features]\n",
        "\n",
        "# Final cleaning: Aggressively clean 'NA' strings and ensure proper types\n",
        "df_cleaned = df_indexed\n",
        "\n",
        "# Clean each numerical column: replace 'NA' strings and ensure double type\n",
        "# Use try_cast to safely handle any remaining 'NA' strings\n",
        "for col_name in numerical_features:\n",
        "    if col_name in df_cleaned.columns:\n",
        "        # Use try_cast to safely convert to double (returns null if 'NA' string)\n",
        "        df_cleaned = df_cleaned.withColumn(\n",
        "            col_name + \"_temp\",\n",
        "            expr(f\"try_cast({col_name} as double)\")  # Safely cast, returns null if 'NA'\n",
        "        ).withColumn(\n",
        "            col_name,\n",
        "            when(col(col_name + \"_temp\").isNull(), 0.0)  # Replace nulls with 0.0\n",
        "            .otherwise(col(col_name + \"_temp\"))\n",
        "        ).drop(col_name + \"_temp\")\n",
        "\n",
        "print(\"✓ Cleaned 'NA' strings and ensured numerical columns are double type\")\n",
        "\n",
        "# Handle missing values manually - calculate mean on clean data\n",
        "df_imputed = df_cleaned\n",
        "\n",
        "print(\"Calculating means and filling missing values...\")\n",
        "for i, col_name in enumerate(numerical_features):\n",
        "    if col_name in df_imputed.columns:\n",
        "        try:\n",
        "            # Calculate mean (this will work now that 'NA' strings are null)\n",
        "            mean_result = df_imputed.select(avg(col(col_name)).alias(\"mean\")).collect()[0]\n",
        "            mean_value = mean_result[\"mean\"] if mean_result[\"mean\"] is not None else 0.0\n",
        "            \n",
        "            # Fill null values with mean\n",
        "            df_imputed = df_imputed.withColumn(\n",
        "                f\"{col_name}_imputed\",\n",
        "                when(col(col_name).isNull(), mean_value)\n",
        "                .otherwise(col(col_name))\n",
        "            )\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(numerical_features)} columns...\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not process {col_name}: {e}\")\n",
        "            # If mean calculation fails, just use 0.0\n",
        "            df_imputed = df_imputed.withColumn(\n",
        "                f\"{col_name}_imputed\",\n",
        "                when(col(col_name).isNull(), 0.0)\n",
        "                .otherwise(col(col_name))\n",
        "            )\n",
        "\n",
        "print(\"✓ Missing values filled with column means\")\n",
        "\n",
        "# Update feature columns to use imputed versions\n",
        "feature_columns_imputed = [f\"{col}_imputed\" for col in numerical_features] + \\\n",
        "                         [f\"{col}_indexed\" for col in categorical_features]\n",
        "\n",
        "print(f\"✓ Features prepared: {len(feature_columns_imputed)} total features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create Feature Vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Features assembled into vector\n",
            "Feature vector size: 28\n"
          ]
        }
      ],
      "source": [
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_columns_imputed,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "df_features = assembler.transform(df_imputed)\n",
        "\n",
        "print(\"✓ Features assembled into vector\")\n",
        "print(f\"Feature vector size: {len(feature_columns_imputed)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying data integrity...\n",
            "✓ Sample check passed (100 records)\n",
            "\n",
            "✓ Data prepared for ML\n",
            "Total records: 119,390\n",
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(28,[0,1,2,3,6,15...|    0|\n",
            "|(28,[0,1,2,3,6,15...|    0|\n",
            "|(28,[0,1,2,3,5,6,...|    0|\n",
            "|(28,[0,1,2,3,5,6,...|    0|\n",
            "|(28,[0,1,2,3,5,6,...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Select final columns for ML\n",
        "# Aggressively clean is_canceled to handle any remaining 'NA' strings\n",
        "from pyspark.sql.functions import expr, regexp_replace\n",
        "\n",
        "# Step 1: Clean is_canceled column - replace any 'NA' variants with null, then cast\n",
        "# This handles cases where 'NA' strings might still exist\n",
        "df_ml = df_features.withColumn(\n",
        "    \"is_canceled_clean\",\n",
        "    # First, replace 'NA' strings with null\n",
        "    when(col(\"is_canceled\").cast(\"string\").isin([\"NA\", \"na\", \"Na\", \"nA\", \"null\", \"NULL\", \"\"]), None)\n",
        "    .otherwise(col(\"is_canceled\"))\n",
        ").withColumn(\n",
        "    \"label_temp\",\n",
        "    # Use try_cast to safely convert to integer\n",
        "    expr(\"try_cast(is_canceled_clean as int)\")\n",
        ").withColumn(\n",
        "    \"label\",\n",
        "    # Convert nulls to 0, ensure integer type\n",
        "    when(col(\"label_temp\").isNull(), 0)\n",
        "    .otherwise(col(\"label_temp\").cast(\"integer\"))\n",
        ").select(\"features\", \"label\")\n",
        "\n",
        "# Step 2: Filter out any rows with null labels (safety check)\n",
        "df_ml = df_ml.filter(col(\"label\").isNotNull())\n",
        "\n",
        "# Step 3: Verify data is clean before caching\n",
        "# Force evaluation to catch any errors early\n",
        "print(\"Verifying data integrity...\")\n",
        "try:\n",
        "    sample_count = df_ml.limit(100).count()\n",
        "    print(f\"✓ Sample check passed ({sample_count} records)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error during verification: {e}\")\n",
        "    print(\"Attempting to filter problematic rows...\")\n",
        "    # Try to filter out any rows that might cause issues\n",
        "    df_ml = df_ml.filter(col(\"label\").isNotNull())\n",
        "    sample_count = df_ml.limit(100).count()\n",
        "    print(f\"✓ After filtering: {sample_count} records\")\n",
        "\n",
        "# Cache for faster access (only after data is verified clean)\n",
        "df_ml.cache()\n",
        "\n",
        "print(\"\\n✓ Data prepared for ML\")\n",
        "print(f\"Total records: {df_ml.count():,}\")\n",
        "df_ml.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Train/Test Split ===\n",
            "Training set: 95,673 records (80.1%)\n",
            "Test set: 23,717 records (19.9%)\n",
            "\n",
            "=== Label Distribution ===\n",
            "Training set:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1|35495|\n",
            "|    0|60178|\n",
            "+-----+-----+\n",
            "\n",
            "\n",
            "Test set:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1| 8729|\n",
            "|    0|14988|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and test sets (80/20)\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"=== Train/Test Split ===\")\n",
        "print(f\"Training set: {train_df.count():,} records ({train_df.count()/df_ml.count()*100:.1f}%)\")\n",
        "print(f\"Test set: {test_df.count():,} records ({test_df.count()/df_ml.count()*100:.1f}%)\")\n",
        "\n",
        "# Check label distribution in both sets\n",
        "print(\"\\n=== Label Distribution ===\")\n",
        "print(\"Training set:\")\n",
        "train_df.groupBy(\"label\").count().show()\n",
        "print(\"\\nTest set:\")\n",
        "test_df.groupBy(\"label\").count().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Processed Data (Optional)\n",
        "\n",
        "Save processed data for use in next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving processed data...\n",
            "✓ Data preprocessing completed\n",
            "\n",
            "✓ Training data saved to: ../data/processed_data/train_data.parquet\n",
            "✓ Test data saved to: ../data/processed_data/test_data.parquet\n",
            "\n",
            "Ready for machine learning models!\n",
            "\n",
            "Next steps:\n",
            "  - Run the next notebook (04_ml_models.ipynb)\n",
            "  - The data will be automatically loaded from saved Parquet files\n",
            "  - Features are in 'features' column\n",
            "  - Labels are in 'label' column\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Save processed data to Parquet for use in next notebook\n",
        "# This allows you to run notebooks separately\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directory for saved data\n",
        "save_dir = \"../data/processed_data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save train and test DataFrames\n",
        "train_path = f\"{save_dir}/train_data.parquet\"\n",
        "test_path = f\"{save_dir}/test_data.parquet\"\n",
        "\n",
        "print(\"Saving processed data...\")\n",
        "train_df.write.mode(\"overwrite\").parquet(train_path)\n",
        "test_df.write.mode(\"overwrite\").parquet(test_path)\n",
        "\n",
        "print(\"✓ Data preprocessing completed\")\n",
        "print(f\"\\n✓ Training data saved to: {train_path}\")\n",
        "print(f\"✓ Test data saved to: {test_path}\")\n",
        "print(\"\\nReady for machine learning models!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  - Run the next notebook (04_ml_models.ipynb)\")\n",
        "print(\"  - The data will be automatically loaded from saved Parquet files\")\n",
        "print(\"  - Features are in 'features' column\")\n",
        "print(\"  - Labels are in 'label' column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✓ Spark session initialized\n",
        "✓ Data loaded and cached\n",
        "✓ Feature engineering completed\n",
        "✓ Missing values handled\n",
        "✓ Categorical features encoded\n",
        "✓ Features assembled into vectors\n",
        "✓ Train/test split performed\n",
        "\n",
        "**Next Steps**: Proceed to `04_ml_models.ipynb` to train machine learning models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
