{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 4: Machine Learning Models\n",
        "\n",
        "This notebook trains multiple machine learning models to predict hotel booking cancellations:\n",
        "1. Naive Bayes\n",
        "2. Decision Tree\n",
        "\n",
        "All models are trained using PySpark MLlib for distributed processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import (\n",
        "    NaiveBayes, DecisionTreeClassifier\n",
        ")\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Spark and Load Data\n",
        "\n",
        "**Note**: This notebook assumes you've run the previous preprocessing notebook. If not, run `03_spark_preprocessing.ipynb` first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or get existing Spark session\n",
        "try:\n",
        "    spark\n",
        "    print(\"✓ Using existing Spark session\")\n",
        "except NameError:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"HotelBookingML\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "    print(\"✓ New Spark session created\")\n",
        "\n",
        "# If you need to reload data, uncomment and run preprocessing steps\n",
        "# Or use the train_df and test_df from previous notebook\n",
        "print(\"\\nNote: Make sure train_df and test_df are available from previous notebook\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Training Functions\n",
        "\n",
        "Define functions to train and evaluate models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_naive_bayes(train_df):\n",
        "    \"\"\"Train Naive Bayes model.\"\"\"\n",
        "    print(\"Training Naive Bayes...\")\n",
        "    nb = NaiveBayes(\n",
        "        featuresCol='features',\n",
        "        labelCol='label'\n",
        "    )\n",
        "    model = nb.fit(train_df)\n",
        "    print(\"✓ Naive Bayes trained\")\n",
        "    return model\n",
        "\n",
        "def train_decision_tree(train_df, max_depth=10):\n",
        "    \"\"\"Train Decision Tree model.\"\"\"\n",
        "    print(\"Training Decision Tree...\")\n",
        "    dt = DecisionTreeClassifier(\n",
        "        featuresCol='features',\n",
        "        labelCol='label',\n",
        "        maxDepth=max_depth,\n",
        "        impurity='gini'\n",
        "    )\n",
        "    model = dt.fit(train_df)\n",
        "    print(\"✓ Decision Tree trained\")\n",
        "    return model\n",
        "\n",
        "print(\"✓ Model training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(predictions, model_name):\n",
        "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
        "    # Binary classification evaluator for AUC\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol='label',\n",
        "        rawPredictionCol='rawPrediction',\n",
        "        metricName='areaUnderROC'\n",
        "    )\n",
        "    \n",
        "    # Multiclass evaluator for other metrics\n",
        "    multiclass_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol='label',\n",
        "        predictionCol='prediction',\n",
        "        metricName='accuracy'\n",
        "    )\n",
        "    \n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'accuracy': multiclass_evaluator.evaluate(predictions),\n",
        "        'auc': binary_evaluator.evaluate(predictions)\n",
        "    }\n",
        "    \n",
        "    # Calculate precision, recall, F1\n",
        "    for metric_name in ['weightedPrecision', 'weightedRecall', 'f1']:\n",
        "        evaluator = MulticlassClassificationEvaluator(\n",
        "            labelCol='label',\n",
        "            predictionCol='prediction',\n",
        "            metricName=metric_name\n",
        "        )\n",
        "        metrics[metric_name] = evaluator.evaluate(predictions)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"✓ Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Model 1 - Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Naive Bayes\n",
        "nb_model = train_naive_bayes(train_df)\n",
        "\n",
        "# Make predictions\n",
        "nb_predictions = nb_model.transform(test_df)\n",
        "\n",
        "# Evaluate\n",
        "nb_metrics = evaluate_model(nb_predictions, \"Naive Bayes\")\n",
        "\n",
        "print(\"\\n=== Naive Bayes Results ===\")\n",
        "for metric, value in nb_metrics.items():\n",
        "    if metric != 'model':\n",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample predictions\n",
        "print(\"\\n=== Sample Predictions ===\")\n",
        "nb_predictions.select(\"label\", \"prediction\", \"probability\").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train Model 2 - Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Decision Tree\n",
        "dt_model = train_decision_tree(train_df, max_depth=10)\n",
        "\n",
        "# Make predictions\n",
        "dt_predictions = dt_model.transform(test_df)\n",
        "\n",
        "# Evaluate\n",
        "dt_metrics = evaluate_model(dt_predictions, \"Decision Tree\")\n",
        "\n",
        "print(\"\\n=== Decision Tree Results ===\")\n",
        "for metric, value in dt_metrics.items():\n",
        "    if metric != 'model':\n",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Decision Tree Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree feature importance (if available)\n",
        "try:\n",
        "    feature_importance = dt_model.featureImportances\n",
        "    print(\"\\n=== Top 10 Most Important Features (Decision Tree) ===\")\n",
        "    # Note: Feature names would need to be mapped from indices\n",
        "    # This is a simplified version\n",
        "    importances = feature_importance.toArray()\n",
        "    top_indices = np.argsort(importances)[-10:][::-1]\n",
        "    for idx in top_indices:\n",
        "        print(f\"Feature {idx}: {importances[idx]:.4f}\")\n",
        "except:\n",
        "    print(\"Feature importance not available for this model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all metrics\n",
        "all_metrics = [nb_metrics, dt_metrics]\n",
        "\n",
        "# Create comparison DataFrame\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "metrics_df = metrics_df.set_index('model')\n",
        "\n",
        "print(\"=== Model Comparison ===\")\n",
        "display(metrics_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions for evaluation notebook\n",
        "# Store predictions in variables for next notebook\n",
        "print(\"✓ All models trained and evaluated\")\n",
        "print(\"\\nModels and predictions available:\")\n",
        "print(\"  - nb_model, nb_predictions, nb_metrics\")\n",
        "print(\"  - dt_model, dt_predictions, dt_metrics\")\n",
        "print(\"  - metrics_df (comparison table)\")\n",
        "\n",
        "# Save metrics to CSV for report\n",
        "metrics_df.to_csv('/content/model_metrics.csv')\n",
        "print(\"\\n✓ Metrics saved to model_metrics.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Summary\n",
        "\n",
        "✓ Naive Bayes trained and evaluated\n",
        "✓ Decision Tree trained and evaluated\n",
        "✓ Model comparison completed\n",
        "\n",
        "**Next Steps**: Proceed to `05_evaluation_visualization.ipynb` for detailed evaluation and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
